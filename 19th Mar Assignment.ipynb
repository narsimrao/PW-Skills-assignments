{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a646ea3-4475-44eb-a107-dd96298315b8",
   "metadata": {},
   "source": [
    "# Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dccd11c-ed0f-4f9b-ba5a-18c061a8677a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986d9b28-f2c6-46dd-87c5-af99fbe94ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Min-Max scaling, also known as normalization, is a data preprocessing technique used to rescale numeric features to a specific range. It transforms the values of the features so that they fall within a predetermined interval, typically between 0 and 1.\n",
    "\n",
    "The formula for Min-Max scaling is:\n",
    "scaled_value = (value - min_value) / (max_value - min_value)\n",
    "\n",
    "Suppose we have a dataset with a feature \"Income\" representing the income levels of individuals. The income values range from $20,000 to $100,000. We want to apply Min-Max scaling to normalize the values between 0 and 1.\n",
    "\n",
    "Original Income values:\n",
    "\n",
    "Income = [20000, 35000, 50000, 80000, 100000]\n",
    "To apply Min-Max scaling, we calculate the minimum and maximum values of the Income feature:\n",
    "\n",
    "min_value = 20000\n",
    "max_value = 100000\n",
    "\n",
    "Using the Min-Max scaling formula, we can transform the values as follows:\n",
    "\n",
    "scaled_value = (value - min_value) / (max_value - min_value)\n",
    "\n",
    "Scaled Income values:\n",
    "\n",
    "Income_scaled = [0.0, 0.15, 0.3, 0.6, 1.0]\n",
    "\n",
    "After applying Min-Max scaling, the income values are now scaled between 0 and 1. The value of 20000 becomes 0, the value of 100000 becomes 1, and the other values are linearly scaled in between. This normalization allows for a fairer comparison and analysis of income levels without the influence of the original scale.\n",
    "\n",
    "By using Min-Max scaling, the income feature is transformed to a common range, making it compatible with other features in the dataset and avoiding the potential dominance of certain features due to their original scales\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f54147-e148-4f8e-a8e6-95243956006b",
   "metadata": {},
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f66f927-d843-4a99-9c18-831a295fb93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32791af-3a48-4023-8f19-66fd1c2fdab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The Unit Vector technique, also known as normalization or vector normalization, is a feature scaling method that transforms the values of a feature to have a unit norm or length. It rescales the feature vector so that its magnitude becomes 1 while preserving the direction of the vector.\n",
    "\n",
    "The formula for Unit Vector scaling is:\n",
    "unit_vector = feature_vector / norm(feature_vector)\n",
    "\n",
    "Here's an example to illustrate its application:\n",
    "\n",
    "Suppose we have a dataset with a feature \"Vector\" representing 2D vectors with (x, y) coordinates. We want to apply Unit Vector scaling to normalize the vectors.\n",
    "\n",
    "Original Vector values:\n",
    "- Vector = [(3, 4), (1, 2), (5, 6)]\n",
    "\n",
    "To apply Unit Vector scaling, we calculate the norm (length) of each vector and divide each vector by its norm:\n",
    "\n",
    "norm = sqrt(x^2 + y^2)\n",
    "\n",
    "Normalized Vector values:\n",
    "- Vector_normalized = [Vector1 / norm(Vector1), Vector2 / norm(Vector2), Vector3 / norm(Vector3)]\n",
    "                    = [(3/5, 4/5), (1/√5, 2/√5), (5/√61, 6/√61)]\n",
    "\n",
    "After applying Unit Vector scaling, the length (norm) of each vector becomes 1. The direction of each vector remains the same, but the magnitude is normalized. This scaling technique is useful when the direction or orientation of the vectors is essential, but the magnitude needs to be standardized.\n",
    "\n",
    "Unit Vector scaling is different from Min-Max scaling (as discussed earlier) because it focuses on normalizing the magnitude of the feature vector rather than scaling the values within a specific range. Min-Max scaling transforms the values to a predetermined interval, typically between 0 and 1, while Unit Vector scaling ensures that the vector length becomes 1.\n",
    "\n",
    "Unit Vector scaling is often employed in scenarios where the magnitude of the feature vectors is not crucial, but their relative directions or angles are important, such as in text classification using word embeddings or similarity-based algorithms like cosine similarity.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab5fa34-be14-4cbc-82bd-e0a10a55db25",
   "metadata": {},
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8645598c-43e2-4828-bdbc-f5ca1a9c727d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2f8ea2-1afa-46ee-9865-c6f53e9b4e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PCA, or Principal Component Analysis, is a dimensionality reduction technique used to transform a high-dimensional dataset into a lower-dimensional representation while retaining most of the important information. It identifies the directions, called principal components, in which the data varies the most and projects the data onto these components.\n",
    "\n",
    "Here's an example to illustrate its application:\n",
    "\n",
    "Suppose we have a dataset with four features: height, weight, age, and income. We want to reduce the dimensionality of the dataset using PCA.\n",
    "\n",
    "Original dataset:\n",
    "```\n",
    "Height   Weight   Age   Income\n",
    "-----------------------------\n",
    "160      60       25    50000\n",
    "170      65       35    60000\n",
    "155      50       28    40000\n",
    "175      70       40    80000\n",
    "```\n",
    "\n",
    "To apply PCA, we first standardize the dataset by subtracting the mean and dividing by the standard deviation to ensure all features have the same scale.\n",
    "\n",
    "Standardized dataset:\n",
    "```\n",
    "Height   Weight   Age    Income\n",
    "------------------------------\n",
    "-0.67    -0.73    -0.61  -0.71\n",
    "0.67     0.37     0.97   0.14\n",
    "-1.34    -1.46    -0.25  -1.42\n",
    "1.34     1.82     0.89   1.99\n",
    "```\n",
    "\n",
    "Next, we perform PCA to identify the principal components. PCA calculates the eigenvectors and eigenvalues of the covariance matrix of the standardized dataset. The eigenvectors represent the directions in which the data varies the most, and the eigenvalues indicate the amount of variance explained by each eigenvector.\n",
    "\n",
    "Let's say after performing PCA, we obtain two principal components: PC1 and PC2.\n",
    "\n",
    "```\n",
    "PC1 = [0.45, 0.49, 0.48, 0.55]\n",
    "PC2 = [-0.75, -0.24, 0.63, -0.05]\n",
    "```\n",
    "\n",
    "We can now project the standardized dataset onto these principal components to obtain the reduced-dimensional representation.\n",
    "\n",
    "Reduced-dimensional dataset:\n",
    "```\n",
    "PC1      PC2\n",
    "------------\n",
    "-1.07    -0.39\n",
    "0.29     0.18\n",
    "-1.95    0.63\n",
    "2.74     -0.42\n",
    "```\n",
    "\n",
    "By applying PCA, we have transformed the original dataset with four features into a reduced-dimensional dataset with two principal components. This reduces the dimensionality while preserving the most significant variations in the data.\n",
    "\n",
    "PCA is widely used in various applications, including image processing, data visualization, and feature extraction. It helps to eliminate redundant or less informative features, improve computational efficiency, and facilitate data exploration and visualization by reducing the data to its essential components.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa25f25-d2be-4dc4-98d4-e7a6a487deae",
   "metadata": {},
   "source": [
    "# Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1dc13b-ca17-4713-a089-f6a61070d6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cb1a50-b6ac-4346-a4fe-d108d37390b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PCA (Principal Component Analysis) can be used for feature extraction, which involves transforming the original features of a dataset into a lower-dimensional representation while retaining important information. In this context, PCA is a technique that identifies the most informative directions, known as principal components, in which the data varies the most and projects the data onto these components. The resulting principal components can then be used as new features in a reduced-dimensional space.\n",
    "\n",
    "Here's an example to illustrate the relationship between PCA and feature extraction:\n",
    "\n",
    "Suppose we have a dataset with several numerical features that describe properties of houses, such as size, number of rooms, age, and price. We want to perform feature extraction using PCA to reduce the dimensionality of the dataset.\n",
    "\n",
    "Original dataset:\n",
    "```\n",
    "Size   Rooms   Age   Price\n",
    "-------------------------\n",
    "1200   3       10    500000\n",
    "1500   4       5     600000\n",
    "1000   2       15    450000\n",
    "1800   5       8     700000\n",
    "```\n",
    "\n",
    "To apply PCA for feature extraction, we first standardize the dataset by subtracting the mean and dividing by the standard deviation to ensure all features have the same scale.\n",
    "\n",
    "Standardized dataset:\n",
    "```\n",
    "Size    Rooms   Age    Price\n",
    "----------------------------\n",
    "-0.47   -0.47   -0.76  -0.62\n",
    "0.47    0.47    -0.12  0.23\n",
    "-1.41   -1.41   -1.39  -0.99\n",
    "1.41    1.41    -0.01  1.38\n",
    "```\n",
    "\n",
    "Next, we perform PCA to identify the principal components. PCA calculates the eigenvectors and eigenvalues of the covariance matrix of the standardized dataset. The eigenvectors represent the directions in which the data varies the most, and the eigenvalues indicate the amount of variance explained by each eigenvector.\n",
    "\n",
    "Let's say after performing PCA, we obtain two principal components: PC1 and PC2.\n",
    "\n",
    "```\n",
    "PC1 = [0.51, 0.51, 0.51, 0.46]\n",
    "PC2 = [0.68, -0.32, -0.68, 0.02]\n",
    "```\n",
    "\n",
    "We can now project the standardized dataset onto these principal components to obtain the reduced-dimensional representation.\n",
    "\n",
    "Reduced-dimensional dataset:\n",
    "```\n",
    "PC1     PC2\n",
    "-----------\n",
    "-0.44   -0.85\n",
    "0.25    0.82\n",
    "-1.95   0.63\n",
    "2.14    -0.60\n",
    "```\n",
    "\n",
    "In this example, PCA has extracted two principal components from the original features: PC1 and PC2. These principal components capture the most significant variations in the data. The reduced-dimensional dataset contains these extracted features, which can now be used as inputs for further analysis or modeling tasks.\n",
    "\n",
    "By using PCA for feature extraction, we have transformed the original dataset with multiple features into a reduced-dimensional representation with a smaller number of principal components. This can help reduce computational complexity, eliminate redundant information, and highlight the most important aspects of the data.\n",
    "\n",
    "Note that the choice of the number of principal components to retain depends on factors like the desired dimensionality reduction, the amount of information explained by the components, and the specific requirements of the analysis or model being built.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c31741d-7aa0-4ea8-a71a-634dc9e45145",
   "metadata": {},
   "source": [
    "# Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c30d5af-4fee-4cab-ac21-2682468c6104",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2374710e-26da-4de0-a10f-f11a83a48ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To preprocess the data for building a recommendation system in a food delivery service, you can use Min-Max scaling. Min-Max scaling is a feature scaling technique that transforms the values of a feature to a specific range, typically between 0 and 1. Here's how you would apply Min-Max scaling to preprocess the features:\n",
    "\n",
    "Determine the minimum and maximum values of each feature in the dataset (e.g., price, rating, delivery time).\n",
    "\n",
    "For each feature, subtract the minimum value and divide by the range (maximum value minus minimum value) of that feature.\n",
    "\n",
    "The resulting values will be scaled between 0 and 1, where 0 represents the minimum value, and 1 represents the maximum value.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa27d7cc-66ee-4787-b26c-e95b459bf451",
   "metadata": {},
   "source": [
    "# Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0509baae-220b-44c4-9309-aecb682ed37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef07e739-47d2-4c33-928b-35535c12f9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To reduce the dimensionality of the dataset in a stock price prediction project, you can use PCA (Principal Component Analysis). PCA is a dimensionality reduction technique that identifies the most informative directions, known as principal components, in which the data varies the most. Here's how you would use PCA to reduce the dimensionality of the dataset:\n",
    "\n",
    "1. Preprocess the data\n",
    "2. Standardize the data\n",
    "3. Perform PCA\n",
    "4. Select the desired number of components\n",
    "5. Project the data onto the selected components\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca4c910-a1fc-4817-9322-322a5fb3d7be",
   "metadata": {},
   "source": [
    "# Q7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ea69c6-ce11-41ba-b367-b0f71dae16c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cca8378-0701-4a11-9a26-86d3b16fec14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.21052632 0.47368421 0.73684211 1.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Original dataset\n",
    "data = np.array([1, 5, 10, 15, 20])\n",
    "\n",
    "# Calculate the minimum and maximum values\n",
    "min_val = np.min(data)\n",
    "max_val = np.max(data)\n",
    "\n",
    "# Calculate the range\n",
    "range_val = max_val - min_val\n",
    "\n",
    "# Perform Min-Max scaling\n",
    "scaled_data = (data - min_val) / range_val\n",
    "\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88276c1-7786-4e85-a069-82d33c413e49",
   "metadata": {},
   "source": [
    "# Q8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67695c5-ed67-42eb-9a6e-a4e99ab59ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d705819b-7022-41d6-8863-29bfb4264c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To determine the number of principal components to retain for feature extraction using PCA, you typically consider the cumulative explained variance and your desired level of dimensionality reduction. Here's a general guideline to help you make this decision:\n",
    "\n",
    "Compute the covariance matrix: Calculate the covariance matrix of the dataset containing the features [height, weight, age, gender, blood pressure].\n",
    "Perform PCA: Apply PCA to the covariance matrix and obtain the eigenvalues and eigenvectors.\n",
    "Sort the eigenvalues: Sort the eigenvalues in descending order.\n",
    "Calculate the explained variance: Calculate the percentage of variance explained by each principal component by dividing each eigenvalue by the sum of all eigenvalues. This gives you the proportion of variance captured by each component.\n",
    "Cumulative explained variance: Calculate the cumulative sum of the explained variances, starting from the first principal component. This tells you the total amount of variance explained as you increase the number of components.\n",
    "Determine the number of components to retain: Choose the number of principal components to retain based on the cumulative explained variance and your desired level of dimensionality reduction. A common threshold is to select the number of components that capture a significant amount of the total variance, such as 95% or 99%.\n",
    "\n",
    "\n",
    "It is difficult to determine the specific number of principal components to retain without knowing the dataset and the desired level of dimensionality reduction. \n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
