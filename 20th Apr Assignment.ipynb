{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9de154c-8a23-48e8-8f4b-bf99ed881b1d",
   "metadata": {},
   "source": [
    "# Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe336e60-7abe-44a5-85e4-f72eec1a672b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "What is the KNN algorithm?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb83abe4-5e45-4d55-9692-e9eaa105c757",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The K-Nearest Neighbors (KNN) algorithm is a non-parametric and supervised machine learning algorithm used for both classification and regression tasks. In KNN, the output of a data point is determined by the majority vote or averaging of its K nearest neighbors in the feature space. The algorithm assumes that similar data points are close to each other in the feature space.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474f9d26-d572-442d-96bd-3d4683b51132",
   "metadata": {},
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0195a6a9-6a94-4258-8921-81c32c5173cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "How do you choose the value of K in KNN?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3604174-f7c7-4e86-a4b5-c70a3bd5cb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The value of K in KNN is a hyperparameter that needs to be chosen before training the model. The choice of K depends on the dataset and the complexity of the problem. A smaller value of K makes the model more sensitive to individual data points, while a larger value of K makes the model smoother but may overlook local patterns. The value of K is typically chosen through experimentation and cross-validation. Common approaches include trying different values of K and evaluating their performance using validation techniques such as k-fold cross-validation.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29cf912-22c8-4619-a96d-e2124eaf6cbc",
   "metadata": {},
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649d32c3-91d0-41e0-b199-804bc2d902c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "What is the difference between KNN classifier and KNN regressor?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b377e3-29f9-4b05-846e-17178ef90380",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "KNN classifier: It is used for classification tasks, where the goal is to assign a data point to one of the predefined classes. In KNN classifier, the output is determined by the majority vote of the K nearest neighbors. The class label with the highest count among the neighbors is assigned to the data point.\n",
    "\n",
    "KNN regressor: It is used for regression tasks, where the goal is to predict a continuous numeric value. In KNN regressor, the output is determined by averaging the target values of the K nearest neighbors. The predicted value is the mean or median of the target values of the neighbors.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af70f94b-9e56-4d79-b90c-69645fb66681",
   "metadata": {},
   "source": [
    "# Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1104dc0-0293-476e-a07c-24eeb92fd280",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "How do you measure the performance of KNN?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9af67e-3980-4149-a1ff-16cda67b36dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The performance of KNN can be measured using various evaluation metrics depending on the task (classification or regression). For classification tasks, common evaluation metrics include accuracy, precision, recall, F1-score, and area under the ROC curve. For regression tasks, common evaluation metrics include mean squared error (MSE), root mean squared error (RMSE), mean absolute error (MAE), and R-squared.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2d8b1b-4c1d-42c7-9551-d7383ad963d9",
   "metadata": {},
   "source": [
    "# Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd3cd66-56eb-4bb0-a909-5dced51d164d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "What is the curse of dimensionality in KNN?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b032ea1-6485-4a66-8bf1-e156da20844e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The curse of dimensionality in KNN refers to the problem that arises when the number of features (dimensions) in the dataset increases. As the number of dimensions increases, the volume of the feature space increases exponentially, making the data points sparse. In high-dimensional spaces, the concept of nearest neighbors becomes less meaningful because the distance between any two points becomes similar. This can lead to degraded performance and inefficiency in KNN as the presence of irrelevant or noisy features can adversely affect the accuracy of predictions.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc3c037-fe22-4332-984d-a412dc7190f0",
   "metadata": {},
   "source": [
    "# Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068db1ab-9a7b-4094-8fbc-41e515ad9c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "How do you handle missing values in KNN?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57449077-699e-4838-9b29-9e2787890441",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Deletion: Remove instances or features with missing values. However, this approach may result in a loss of valuable information if the missing values are not random.\n",
    "\n",
    "Imputation: Fill in missing values with estimated values. For numerical features, common imputation methods include using the mean, median, or regression-based predictions. For categorical features, the missing values can be replaced with the most frequent category.\n",
    "\n",
    "KNN Imputation: Predict the missing values based on the values of the nearest neighbors. For each missing value, the algorithm finds the K nearest neighbors and takes the average (for numerical features) or the majority vote (for categorical features) of their values.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2007178e-f2be-403f-bc07-4c7fd13fb3ee",
   "metadata": {},
   "source": [
    "# Q7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad83469-140d-4c75-a9d2-94f29993f8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e51a2a-05b3-478e-ac79-fef287ead4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The performance of the KNN classifier and regressor depends on the nature of the problem:\n",
    "\n",
    "KNN Classifier:\n",
    "\n",
    "Suitable for classification tasks where the target variable consists of discrete classes or categories.\n",
    "Makes predictions based on the majority vote of the K nearest neighbors.\n",
    "Evaluation metrics for classification include accuracy, precision, recall, F1-score, and area under the ROC curve.\n",
    " \n",
    "\n",
    "KNN Regressor:\n",
    "\n",
    "Suitable for regression tasks where the target variable is a continuous numerical value.\n",
    "Makes predictions by averaging the target values of the K nearest neighbors.\n",
    "Evaluation metrics for regression include mean squared error (MSE), root mean squared error (RMSE), mean absolute error (MAE), and R-squared.\n",
    "\n",
    "\n",
    "It is important to choose the appropriate algorithm based on the problem at hand. If the target variable is categorical, KNN classifier is used, while for numerical target variables, KNN regressor is employed.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc3f273-b3f0-4f61-bd25-8017ec9f2f09",
   "metadata": {},
   "source": [
    "# Q8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cf8064-135c-44ff-b997-4e12c2034f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916d11ca-6038-4d83-aee4-4625ca8b083d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Strengths of KNN:\n",
    "\n",
    "Simple and easy to understand.\n",
    "Does not require assumptions about the underlying data distribution.\n",
    "Effective in capturing complex relationships in the data.\n",
    "\n",
    "\n",
    "Weaknesses of KNN:\n",
    "\n",
    "Computationally expensive when dealing with large datasets.\n",
    "Sensitive to the choice of K and the distance metric.\n",
    "Requires feature scaling for distance-based calculations.\n",
    "Can be biased towards features with higher scales.\n",
    "\n",
    "\n",
    "To address these weaknesses:\n",
    "\n",
    "KNN can benefit from dimensionality reduction techniques to reduce the computational complexity.\n",
    "Choosing an appropriate value of K through cross-validation can improve performance.\n",
    "Selecting the right distance metric, such as Euclidean or Manhattan, based on the problem and data characteristics can lead to better results.\n",
    "Feature scaling should be performed to normalize the feature values and avoid the dominance of certain features based on scale.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b616c6-2382-484f-b727-0a027139fd1e",
   "metadata": {},
   "source": [
    "# Q9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca21613-81dc-4909-95a4-75e99bc1a955",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1726dd86-8c8f-49bc-8ad2-235db89dc786",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Euclidean distance and Manhattan distance are two commonly used distance metrics in KNN:\n",
    "\n",
    "Euclidean distance: It calculates the straight-line or Euclidean distance between two points in a multidimensional space. In a 2D space, the Euclidean distance between points (x1, y1) and (x2, y2) is given by the formula: √((x2 - x1)^2 + (y2 - y1)^2). In higher dimensions, it extends similarly. Euclidean distance considers the magnitude of the differences between the coordinates.\n",
    "\n",
    "Manhattan distance: It calculates the distance between two points by summing the absolute differences between their coordinates. In a 2D space, the Manhattan distance between points (x1, y1) and (x2, y2) is given by |x2 - x1| + |y2 - y1|. It is called Manhattan distance because it measures the distance a taxi would have to travel along the city blocks (right angles) to reach the destination. Manhattan distance only considers the absolute differences between the coordinates.\n",
    "\n",
    "The choice between Euclidean and Manhattan distance depends on the data and the problem at hand. Euclidean distance is commonly used when the features have continuous and normalized values, while Manhattan distance is useful when dealing with categorical or ordinal features.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdaf5a8-edf6-47b9-b520-a2b8332cee5a",
   "metadata": {},
   "source": [
    "# Q10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0755da6d-acfe-4e82-b475-68cac76c44fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "What is the role of feature scaling in KNN?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d8f3fa-ba4e-4a1c-a5e8-fa7f51e98eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Feature scaling plays an important role in KNN because it helps in comparing and calculating distances between data points. KNN uses the distance between points to determine their similarity and select the nearest neighbors.\n",
    "\n",
    "When features have different scales or units, those with larger scales can dominate the distance calculations, leading to biased results. Feature scaling ensures that all features are on a similar scale, giving equal importance to each feature during distance calculations.\n",
    "\n",
    "Common methods of feature scaling include:\n",
    "\n",
    "Min-Max scaling (Normalization): It scales the feature values to a specific range, usually between 0 and 1, by subtracting the minimum value and dividing by the range (maximum - minimum).\n",
    "Standardization: It transforms the feature values to have a mean of 0 and a standard deviation of 1 by subtracting the mean and dividing by the standard deviation.\n",
    "\n",
    "\n",
    "Applying feature scaling to the dataset before performing KNN helps to prevent any feature from dominating the distance calculations and ensures that all features contribute equally to the final predictions.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
