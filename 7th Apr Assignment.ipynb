{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7406ac31-d795-44bc-8746-6d4fa803452c",
   "metadata": {},
   "source": [
    "# Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e6434b-e97e-4034-a8f6-3e68a7e09254",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "What is the relationship between polynomial functions and kernel functions in machine learning algorithms?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6740b2cd-485f-4470-a180-692344203477",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In machine learning algorithms, kernel functions play a crucial role in transforming the input data into a higher-dimensional feature space. Polynomial functions are one type of kernel function commonly used in machine learning algorithms, including Support Vector Machines (SVMs). A polynomial kernel function computes the similarity between two data points in terms of the polynomial degree.\n",
    "\n",
    "Polynomial kernel functions are defined as K(x, y) = (α * x^T * y + c)^d, where x and y are input data points, α and c are parameters, and d is the degree of the polynomial. The kernel function computes the inner product between the transformed feature vectors in the higher-dimensional space, without explicitly computing the transformation.\n",
    "\n",
    "The polynomial kernel allows SVMs to handle non-linear decision boundaries by implicitly mapping the data into a higher-dimensional space, where linear separation becomes possible. The degree of the polynomial determines the complexity of the decision boundary, with higher degrees allowing for more complex and flexible decision boundaries.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6002188-336a-4a35-9158-71b8aa7359e2",
   "metadata": {},
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89d64a4-7536-4019-97fb-b996850d9922",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47347b31-a836-4730-81e4-ce0b1ba2d686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an instance of SVC with a polynomial kernel\n",
    "svm = SVC(kernel='poly', degree=3)\n",
    "\n",
    "# Train the SVM on the training data\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Use the trained SVM to predict labels for the testing data\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the SVM\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f01ef7-0fb6-4674-a6d7-170d9a0109c5",
   "metadata": {},
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fed1ba6-bbda-48bf-81eb-4927aa80bf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "How does increasing the value of epsilon affect the number of support vectors in SVR?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55665ba4-2f9b-4bfd-9331-d0bcb91470fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In Support Vector Regression (SVR), epsilon is a hyperparameter that controls the width of the margin and the number of support vectors. The margin represents the region within which errors are tolerated. Increasing the value of epsilon allows for a wider margin and a larger number of support vectors.\n",
    "\n",
    "Support vectors are the data points that lie on the margin or within the margin boundaries. They are critical for defining the regression function in SVR. As epsilon increases, the margin becomes wider, allowing more data points to fall within it. Consequently, the number of support vectors also increases.\n",
    "\n",
    "Increasing epsilon relaxes the tolerance for errors, allowing the SVR model to fit the training data more loosely. This can lead to a more flexible and less accurate model, as the wider margin accommodates more data points, including potential outliers or noisy observations.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9976987-20b9-4229-af0c-05bcc0597551",
   "metadata": {},
   "source": [
    "# Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2534ca4c-fdd1-4f58-9ffb-c7e9361d7f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works and provide examples of when you might want to increase or decrease its value?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c6d9af-33b6-44bb-a2b6-20d317e108c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Kernel Function: The choice of the kernel function determines the mapping of the data to a higher-dimensional space. Different kernel functions have different characteristics and are suitable for different types of data. For example, the linear kernel is useful for linear data, while the radial basis function (RBF) kernel is more flexible and suitable for non-linear data. Choosing the right kernel function depends on the specific problem and the underlying data distribution.\n",
    "\n",
    "C Parameter: The C parameter is a regularization parameter that controls the trade-off between achieving a low training error and a low complexity (smoothness) of the regression function. A smaller C value allows for more errors and a smoother model, while a larger C value penalizes errors more heavily, leading to a more complex and potentially overfitting model. Increasing C may be appropriate when the training data is believed to have less noise or outliers and a higher degree of complexity is desired.\n",
    "\n",
    "Epsilon Parameter: The epsilon parameter in SVR defines the margin around the predicted values within which errors are tolerated. It determines the width of the epsilon-insensitive tube. Increasing epsilon allows for a wider margin and larger errors that are considered acceptable. This can lead to a more flexible model that fits the training data loosely. Decreasing epsilon makes the margin narrower and requires predictions to be closer to the true values. Choosing epsilon depends on the acceptable level of error in the regression model.\n",
    "\n",
    "Gamma Parameter: The gamma parameter affects the shape and reach of the influence of each training example. It defines the scale of the Gaussian (RBF) kernel and determines how far the influence of a single training example reaches. A small gamma value implies a broader influence and results in a smoother decision boundary. On the other hand, a large gamma value focuses on the nearest neighbors, leading to a more complex decision boundary with tighter fits. Increasing gamma can be appropriate when the training data is believed to have more complex relationships or when overfitting is a concern.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad99f23-9a4e-4523-b288-f11cab115949",
   "metadata": {},
   "source": [
    "# Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b4c7e7-c83b-410c-9fcd-d27d2a419214",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Assignment\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5eb91e31-02e7-493b-b675-86b892600c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Best Hyperparameters: {'C': 10, 'gamma': 0.1, 'kernel': 'linear'}\n",
      "Best Score: 0.9583333333333334\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['svm_classifier.pkl']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "\n",
    "# Load the dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess the data (e.g., scaling)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create an instance of the SVC classifier\n",
    "svm = SVC()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Use the trained classifier to predict labels for the testing data\n",
    "y_pred = svm.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the performance of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf']\n",
    "}\n",
    "\n",
    "# Perform grid search to find the best hyperparameters\n",
    "grid_search = GridSearchCV(svm, param_grid, cv=5)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best hyperparameters and score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)\n",
    "\n",
    "# Train the tuned classifier on the entire dataset\n",
    "svm_tuned = SVC(**best_params)\n",
    "svm_tuned.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Save the trained classifier to a file\n",
    "joblib.dump(svm_tuned, 'svm_classifier.pkl')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
