{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c60d722-d679-4d8b-9b13-8b9eac650d93",
   "metadata": {},
   "source": [
    "# Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9449eb-ea90-4bf8-9e22-b92724aaf994",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "What is the mathematical formula for a linear SVM?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efde1af-6645-499d-a94d-2e10124303e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "f(x) = sign(w^T x + b)\n",
    "\n",
    "where:\n",
    "- f(x) is the predicted class label for input x,\n",
    "- w is the weight vector,\n",
    "- b is the bias term,\n",
    "- x is the input vector.\n",
    "\n",
    "The sign function returns +1 for positive values and -1 for negative values, effectively classifying the input into one of the two classes.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c13a16-4ef8-4dd3-a5b9-233836fdf52b",
   "metadata": {},
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e3a124-f96a-4b7f-89e1-d283cce09116",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "What is the objective function of a linear SVM?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb881faa-8d4f-47bd-923d-8d375d3a2199",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "minimize: (1/2) ||w||^2\n",
    "subject to: y_i(w^T x_i + b) >= 1, for all training samples (x_i, y_i)\n",
    "\n",
    "where:\n",
    "- w is the weight vector,\n",
    "- b is the bias term,\n",
    "- x_i is the i-th training sample,\n",
    "- y_i is the corresponding class label (+1 or -1).\n",
    "\n",
    "The objective is to find the values of w and b that minimize the norm of the weight vector while ensuring that all training samples lie outside a margin of width 1.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3ec8ba-2045-4660-b22c-f02a8b343c4a",
   "metadata": {},
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157f5862-8a59-459b-b78f-9b531e82ab7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "What is the kernel trick in SVM?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c7d2d9-4f35-4d6d-89a7-d0f8c9312f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The kernel trick in SVM is a technique that allows SVM to efficiently handle non-linearly separable data without explicitly mapping the data into a higher-dimensional feature space. Instead of computing the transformation explicitly, the kernel trick defines a kernel function that operates directly on the original data.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645e222e-2e1d-49ae-8f8b-3d41ce1b120c",
   "metadata": {},
   "source": [
    "# Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef812580-de00-486d-a8a2-bae2d7adc95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "What is the role of support vectors in SVM Explain with example\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefdef90-ff4b-4457-9c46-cd74bbebf4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The support vectors in SVM are the data points from the training set that lie closest to the decision boundary. They are the critical samples that determine the position and orientation of the decision boundary. The support vectors play a crucial role in SVM for two main reasons:\n",
    "\n",
    "1. Support vectors define the decision boundary: The decision boundary of SVM is determined by the support vectors. The remaining training samples that are not support vectors do not affect the decision boundary.\n",
    "\n",
    "2. Support vectors are important for generalization: The support vectors are the most influential samples for the SVM model. Their position and orientation relative to the decision boundary are crucial for the model's ability to generalize to unseen data. Removing or changing support vectors can significantly impact the model's performance.\n",
    "\n",
    "Example: Consider a binary classification problem with two classes, where the classes are not linearly separable. In SVM, only a subset of the training samples, which are the support vectors, will be located near the decision boundary. These support vectors define the margin and contribute to the calculation of the decision boundary. The remaining samples, which are not support vectors, are not as important for the model's decision-making process.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112aa97c-5e26-4199-a253-4c5f5e86a90b",
   "metadata": {},
   "source": [
    "# Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b80a8e-87a7-42e4-9a17-ffdd2db65e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in SVM?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ad60c4-5e4c-4d4a-ad27-9379a27502b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- Hyperplane: In SVM, a hyperplane is a decision boundary that separates the classes. For a binary classification problem, the hyperplane is a (d-1)-dimensional subspace in the d-dimensional feature space. In a linear SVM, the hyperplane is a linear function defined by the weight vector w and bias term b.\n",
    "\n",
    "- Marginal plane: The marginal plane refers to the planes that are parallel to the hyperplane and pass through the support vectors. These planes define the margins of the SVM model. The distance between the hyperplane and the marginal planes is known as the margin. The optimal hyperplane is the one that maximizes the margin.\n",
    "\n",
    "- Soft margin: In SVM, a soft margin allows for some misclassification of the training samples. It introduces a slack variable to allow for a certain degree of error or overlap between the classes. Soft margin SVM is used when the data is not linearly separable, and the objective is to find a decision boundary that achieves a balance between maximizing the margin and allowing misclassification.\n",
    "\n",
    "- Hard margin: In SVM, a hard margin does not allow any misclassification of the training samples. It assumes that the data is linearly separable, and the objective is to find a decision boundary that perfectly separates the classes. Hard margin SVM is used when the data is linearly separable.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccf0801-85ca-44dd-8db3-d358000ae63f",
   "metadata": {},
   "source": [
    "# Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c936de8-b902-4cd5-a7e4-34bae3e00280",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SVM Implementation through Iris dataset.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d04794e-d3ec-457b-a170-478465af0d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an SVM classifier\n",
    "svm = SVC(kernel='linear')\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d361f2-d434-4884-9e49-69086d75fda5",
   "metadata": {},
   "source": [
    "# Bonus Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7298cb62-eb9e-4140-9620-c60c931f0fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implement a linear SVM classifier from scratch using Python and compare its performance with the scikit-learn implementation.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc7e53bd-0fa7-4d4f-891f-61ba2d611974",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LinearSVM:\n",
    "    def __init__(self, learning_rate=0.001, num_epochs=1000, C=1.0):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.C = C\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        num_samples, num_features = X.shape\n",
    "        \n",
    "        # Initialize weights and bias\n",
    "        self.weights = np.zeros(num_features)\n",
    "        self.bias = 0\n",
    "        \n",
    "        # Training loop\n",
    "        for _ in range(self.num_epochs):\n",
    "            # Compute scores and predicted labels\n",
    "            scores = np.dot(X, self.weights) + self.bias\n",
    "            y_pred = np.where(scores >= 0, 1, -1)\n",
    "            \n",
    "            # Compute hinge loss\n",
    "            hinge_loss = np.maximum(0, 1 - y * scores)\n",
    "            \n",
    "            # Compute gradient\n",
    "            dW = np.zeros(num_features)\n",
    "            for idx, loss in enumerate(hinge_loss):\n",
    "                if loss > 0:\n",
    "                    dW += self.C * y[idx] * X[idx]\n",
    "            \n",
    "            # Update weights and bias\n",
    "            self.weights -= self.learning_rate * (self.weights - dW)\n",
    "            self.bias -= self.learning_rate * np.sum(-self.C * y * (hinge_loss > 0))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        scores = np.dot(X, self.weights) + self.bias\n",
    "        y_pred = np.where(scores >= 0, 1, -1)\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66ee867e-a911-49ca-af42-9d41d8517e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Scratch Implementation): 0.3\n",
      "Accuracy (Scikit-learn Implementation): 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Instantiate and train the scratch implementation\n",
    "svm_scratch = LinearSVM()\n",
    "svm_scratch.fit(X_train, y_train)\n",
    "y_pred_scratch = svm_scratch.predict(X_test)\n",
    "\n",
    "# Instantiate and train the scikit-learn implementation\n",
    "svm_sklearn = LinearSVC()\n",
    "svm_sklearn.fit(X_train, y_train)\n",
    "y_pred_sklearn = svm_sklearn.predict(X_test)\n",
    "\n",
    "# Compare the accuracy of both implementations\n",
    "accuracy_scratch = accuracy_score(y_test, y_pred_scratch)\n",
    "accuracy_sklearn = accuracy_score(y_test, y_pred_sklearn)\n",
    "\n",
    "print(\"Accuracy (Scratch Implementation):\", accuracy_scratch)\n",
    "print(\"Accuracy (Scikit-learn Implementation):\", accuracy_sklearn)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
