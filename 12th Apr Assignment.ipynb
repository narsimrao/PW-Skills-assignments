{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb0aae43-958f-4666-981f-4b3612d035f9",
   "metadata": {},
   "source": [
    "# Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64af269d-ff0b-460a-b6b7-20ee8d4809ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "How does bagging reduce overfitting in decision trees?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58c4a8a-957c-40ab-a996-624bbefc1ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Bagging reduces overfitting in decision trees by creating multiple bootstrap samples from the original training data and training each tree on a different sample. This introduces diversity in the training process and reduces the likelihood of overfitting to specific patterns or outliers present in the data. By aggregating the predictions of multiple trees, bagging helps to generalize better to unseen data.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5421e808-f6d6-4344-bb8b-e0cc62ac5f83",
   "metadata": {},
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2f75e6-752e-412c-8dd2-566e19b5228f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "What are the advantages and disadvantages of using different types of base learners in bagging?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0facba1-7843-44ef-b95c-b95bbd20ad59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Advantages: Using diverse base learners can enhance the diversity of the ensemble and capture different aspects of the data. Different base learners may have different strengths in handling specific patterns or noise in the data, leading to improved overall performance.\n",
    "\n",
    "Disadvantages: The choice of base learners also brings considerations such as complexity and interpretability. Some base learners may be computationally expensive, making training and prediction time-consuming. Additionally, complex base learners can introduce higher model complexity and may be harder to interpret compared to simpler models.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaea4a34-0c47-4fd8-a052-005cd3328565",
   "metadata": {},
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e711f1d-8740-4f71-9347-774aec6991cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "How does the choice of base learner affect the bias-variance tradeoff in bagging?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a40bbd-fb99-4387-b0fe-58df7ee4f051",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The choice of base learner affects the bias-variance tradeoff in bagging. If the base learner has high bias (e.g., decision stumps or linear models), bagging can help reduce bias by combining multiple weak models. On the other hand, if the base learner has high variance (e.g., deep decision trees or complex models), bagging can help reduce variance by averaging the predictions of multiple models. Overall, bagging tends to decrease variance more than bias, but the impact on the bias-variance tradeoff depends on the specific characteristics of the base learner.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910b1444-059a-4b63-8e32-c55d4b6b37cd",
   "metadata": {},
   "source": [
    "# Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa135d4-ea64-40e1-bde4-a6964d141869",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Can bagging be used for both classification and regression tasks? How does it differ in each case?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ef07fc-1215-4607-b391-c2a987336df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Yes, bagging can be used for both classification and regression tasks.\n",
    "\n",
    "Classification: In classification tasks, each base learner in the ensemble is typically trained to predict the class labels of the target variable. The final prediction is often determined by majority voting, where the class label with the highest frequency among the predictions is chosen.\n",
    "\n",
    "Regression: In regression tasks, each base learner predicts a continuous value. The final prediction can be obtained by averaging the predictions of all base learners. Alternatively, weighted averaging can be used, where each base learner's prediction is weighted based on its performance or other criteria.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46903475-f3b0-4ef4-90e7-5cbcb016115b",
   "metadata": {},
   "source": [
    "# Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d0304b-410c-416a-86ed-f43efaa0aaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "What is the role of ensemble size in bagging? How many models should be included in the ensemble?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b0217d-b0d3-4e81-9b66-e9f88acabefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The ensemble size in bagging refers to the number of base learners (decision trees) included in the ensemble. The ensemble size should be large enough to capture sufficient diversity among the base learners, but adding more models beyond a certain point may not yield significant improvement and can lead to increased computational resources and training time. The optimal ensemble size can vary depending on the dataset and problem complexity. Typically, a rule of thumb is to use a large enough ensemble, such as 100 or more trees, to achieve stable and reliable results.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f3ea20-4d2e-4910-92cd-fe22daa8f386",
   "metadata": {},
   "source": [
    "# Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f7768e-c066-41e7-a329-e7e228b006ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Can you provide an example of a real-world application of bagging in machine learning?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9d10e6-c5bc-4dbc-9826-afa126b7c627",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A real-world application of bagging in machine learning is in the field of medical diagnosis. Bagging can be used to create an ensemble of decision trees trained on different samples of patient data, where each tree predicts the likelihood of a particular disease or condition. By aggregating the predictions of multiple trees, the bagging ensemble can provide a more robust and accurate diagnosis, reducing the impact of individual decision tree errors or biases. This can help healthcare professionals make better-informed decisions and improve patient outcomes.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
