{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87ba9e9b-aa04-4ea2-b257-7596f862d930",
   "metadata": {},
   "source": [
    "# Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8d05a3-064e-4758-9bf4-3e600dde37a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "What is Gradient Boosting Regression?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fbbe43-5d34-416f-a603-86baeed6901f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Gradient Boosting Regression is a machine learning algorithm that combines multiple weak learners (usually decision trees) to create a strong predictive model for regression tasks. It works by iteratively training the weak learners on the residuals (the differences between the actual target values and the predicted values from previous learners) to improve the predictions at each iteration. The final prediction is obtained by summing the predictions of all weak learners, each multiplied by a learning rate.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea5aadc-eda8-423e-9f69-1cb6e1b7aca5",
   "metadata": {},
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d8d965-28fc-4029-be74-96e849386da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41786b72-4d19-4bb0-92fa-c0a6eb651dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.9726132367245546\n",
      "R-squared: 0.8784233454094307\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Create a toy dataset\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([2, 4, 6, 8, 10])\n",
    "\n",
    "# Train the gradient boosting regressor\n",
    "regressor = GradientBoostingRegressor(n_estimators=10, max_depth=3, learning_rate=0.1)\n",
    "regressor.fit(X, y)\n",
    "\n",
    "# Make predictions\n",
    "predictions = regressor.predict(X)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "mse = mean_squared_error(y, predictions)\n",
    "r2 = r2_score(y, predictions)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0697fdce-014b-42ef-a98e-2d3f68b815c4",
   "metadata": {},
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15d1ef1-0503-4e34-b1ce-3a980881ba0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f11a394c-cbe0-426d-be1f-9846c0bf0765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Mean Squared Error: 0.9726132367245546\n",
      "Best R-squared: 0.8784233454094307\n",
      "Best Hyperparameters: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 10}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.1, 0.01, 0.001]\n",
    "}\n",
    "\n",
    "# Create the grid search object\n",
    "grid_search = GridSearchCV(GradientBoostingRegressor(), param_grid, cv=5)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "best_regressor = GradientBoostingRegressor(**best_params)\n",
    "best_regressor.fit(X, y)\n",
    "\n",
    "# Make predictions\n",
    "best_predictions = best_regressor.predict(X)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "best_mse = mean_squared_error(y, best_predictions)\n",
    "best_r2 = r2_score(y, best_predictions)\n",
    "\n",
    "print(\"Best Mean Squared Error:\", best_mse)\n",
    "print(\"Best R-squared:\", best_r2)\n",
    "print(\"Best Hyperparameters:\", best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ced6194-4f35-4046-b24d-084ca4ae4dfc",
   "metadata": {},
   "source": [
    "# Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddd9643-41ff-4c86-b81f-ee4b3926f116",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "What is a weak learner in Gradient Boosting?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e609f2-7a84-4ad2-a964-17db2cf16595",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A weak learner in Gradient Boosting is a simple and relatively low-complexity model that performs slightly better than random guessing on a given task. In the context of Gradient Boosting, weak learners are often decision trees with a small maximum depth. These decision trees are trained to capture patterns and make predictions slightly better than chance, and they are combined to form a strong ensemble model.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0315ec9a-c72d-460d-a7df-2d3dde5edffd",
   "metadata": {},
   "source": [
    "# Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60363fee-49ea-4ce5-b37b-eeb17e22972f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "What is the intuition behind the Gradient Boosting algorithm?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c399559-5cf7-4d03-ba5e-c8d3afab9f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The intuition behind the Gradient Boosting algorithm is to iteratively build an ensemble of weak learners, where each subsequent weak learner focuses on the mistakes made by the previous ones. The algorithm starts by training a weak learner on the original dataset and then trains additional weak learners on the residuals (the differences between the actual target values and the predictions of the previous weak learners). By combining the predictions of multiple weak learners, each targeting different aspects of the data, the algorithm aims to create a strong learner that can capture complex relationships and improve prediction accuracy.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28628a37-f662-4937-b141-65a719507049",
   "metadata": {},
   "source": [
    "# Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46048bc5-9819-45ca-8fcd-7f1f16589ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce6df42-eb95-4c05-a1d6-bc2b4403cb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The Gradient Boosting algorithm builds an ensemble of weak learners in an iterative manner. It starts by training an initial weak learner (usually a decision tree) on the original dataset. Then, for each subsequent iteration, the algorithm trains a new weak learner on the residuals (the differences between the actual target values and the predictions of the previous weak learners). The predictions from all weak learners are combined by adding them together, each multiplied by a learning rate, to obtain the final prediction of the ensemble.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54994b2-2e1e-4297-9dc2-f25362ce824b",
   "metadata": {},
   "source": [
    "# Q7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f001f1c-d6b1-4c95-9412-df91a8d3198e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053f755e-c086-4c3b-84b4-2e71364ee7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "the Gradient Boosting algorithm involves the following steps:\n",
    "\n",
    "* Initialize the prediction: Start with an initial prediction, which can be a simple estimate such as the mean or median of the target variable.\n",
    "* Compute the residuals: Calculate the differences between the actual target values and the predictions obtained so far.\n",
    "* Train a weak learner: Fit a weak learner (e.g., decision tree) to the residuals, aiming to predict the residuals.\n",
    "* Update the predictions: Add the predictions of the weak learner, multiplied by a learning rate, to the previous predictions.\n",
    "* Repeat steps 2-4: Compute new residuals, train another weak learner on the residuals, and update the predictions.\n",
    "* Combine the predictions: Sum up the predictions from all weak learners, each multiplied by the learning rate, to obtain the final prediction of the ensemble.\n",
    "* Repeat steps 2-6 for multiple iterations to improve the predictions and reduce the residuals.\n",
    "\n",
    "The algorithm iteratively trains weak learners to capture the patterns that were missed by the previous learners and combines their predictions to create a strong ensemble model.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
