{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3e1114e-c4ab-477f-96fe-78aa3416a4ee",
   "metadata": {},
   "source": [
    "# Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17568841-4652-4687-84f9-eea432599a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "What is Lasso Regression, and how does it differ from other regression techniques?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec7eb1d-18b9-4e10-b6b4-5c775bd00c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Lasso Regression, short for Least Absolute Shrinkage and Selection Operator, is a regression technique used for both prediction and feature selection. It differs from other regression techniques, such as ordinary least squares (OLS) regression, by adding an L1 norm penalty term to the cost function. This penalty term encourages sparsity in the coefficient estimates by driving some of them exactly to zero. Lasso Regression can effectively select relevant features and shrink the coefficients towards zero, providing a more interpretable and sparse model.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc73faa9-4df2-4ae5-acd3-9d0aa9ae54fa",
   "metadata": {},
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5968ab6d-00f3-4625-8096-89e1ee357e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "What is the main advantage of using Lasso Regression in feature selection?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9218016-92c1-4637-a51c-c431b7cb34b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The main advantage of using Lasso Regression in feature selection is its ability to drive some coefficients to exactly zero. This means that Lasso Regression can automatically identify and eliminate irrelevant predictors from the model, resulting in a sparse solution with only the most important predictors retained. This feature selection property of Lasso Regression is valuable in high-dimensional datasets with many potential predictors, as it helps to improve interpretability, reduce overfitting, and enhance predictive performance.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5de009-b81a-4c34-a402-43303f74bf9a",
   "metadata": {},
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8153d4f-c386-4422-8c7c-84d25949df5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "How do you interpret the coefficients of a Lasso Regression model?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61342299-44bf-4a34-a1f0-8dacba2eeff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The coefficients of a Lasso Regression model can be interpreted similarly to those in ordinary linear regression. Each coefficient represents the change in the dependent variable associated with a one-unit change in the corresponding predictor, assuming all other predictors are held constant. However, due to the L1 norm penalty in Lasso Regression, some coefficients may be exactly zero, indicating that the corresponding predictors have been effectively excluded from the model. The non-zero coefficients indicate the magnitude and direction of the relationships between the predictors and the dependent variable.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0dbd66-dd64-4a7e-871f-10384b492655",
   "metadata": {},
   "source": [
    "# Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffefd9f-96f0-4100-b1a9-5e8fd0fbdba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00eef47-04f5-435d-9a4b-b702d8b0fe76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The tuning parameter in Lasso Regression is often denoted as lambda (λ) or alpha (α). It controls the amount of shrinkage applied to the coefficients. A higher value of lambda leads to more shrinkage and increases the sparsity of the solution, resulting in more coefficients being driven to zero. Conversely, a lower value of lambda reduces the amount of shrinkage and allows more predictors to have non-zero coefficients. The tuning parameter lambda balances the trade-off between model complexity and prediction accuracy, and its optimal value can be determined using techniques such as cross-validation or grid search.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d54775-53aa-43fa-990a-ebc459a18855",
   "metadata": {},
   "source": [
    "# Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d92ad31-fcef-44b8-a871-2fe260ab2af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Can Lasso Regression be used for non-linear regression problems? If yes, how?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8545f5-e6a5-4ed0-aa76-1f12ffda1cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Lasso Regression can handle non-linear regression problems indirectly by incorporating non-linear transformations of the predictors into the model. By including non-linear terms, such as polynomial features or interaction terms, Lasso Regression can capture non-linear relationships between the predictors and the dependent variable. However, it's important to note that the non-linearity is achieved through the transformation of the input features rather than through explicit non-linear functions within the model. If the non-linear relationship is complex and cannot be adequately captured by feature transformations, alternative non-linear regression techniques may be more appropriate.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f339c86f-08ef-4b4b-b1b8-a2d4ee4d1a76",
   "metadata": {},
   "source": [
    "# Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db96447f-6ca0-4692-b3bb-500b1960c99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "What is the difference between Ridge Regression and Lasso Regression?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dcb9c6-37a7-41c5-a28f-3e808596730b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The main difference between Ridge Regression and Lasso Regression lies in the type of penalty term used. Ridge Regression adds an L2 norm penalty term to the cost function, which leads to the shrinking of coefficient magnitudes towards zero without driving them exactly to zero. In contrast, Lasso Regression adds an L1 norm penalty term, which not only shrinks the coefficients but can also drive some of them exactly to zero. As a result, Lasso Regression performs feature selection by automatically excluding irrelevant predictors, while Ridge Regression retains all predictors in the model but reduces their impact.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a88a381-d07f-4f1f-b4a5-f1a766fc1536",
   "metadata": {},
   "source": [
    "# Q7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680a63e7-9131-4d65-aa3f-1eab00957884",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Can Lasso Regression handle multicollinearity in the input features? If yes, how?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee62011-e83f-4fae-a123-736deb2ddedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Yes, Lasso Regression can handle multicollinearity in the input features. In fact, it can effectively deal with multicollinearity by driving some correlated predictors to zero. The L1 norm penalty in Lasso Regression encourages sparsity and promotes feature selection, which naturally addresses the issue of multicollinearity. By eliminating irrelevant predictors, Lasso Regression reduces the impact of correlated predictors on the model and provides more stable coefficient estimates compared to ordinary linear regression.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85058fb1-4a6a-4764-85ea-4d7fa1cd65bc",
   "metadata": {},
   "source": [
    "# Q8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bfade8-69a5-4335-88a4-914972884fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dac5db3-5b14-422a-b7de-e5cadcbd96f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The optimal value of the regularization parameter (lambda) in Lasso Regression can be determined using techniques such as cross-validation or grid search. Cross-validation involves splitting the dataset into multiple subsets, training the Lasso Regression model on different combinations of training and validation subsets, and evaluating the model's performance using a chosen metric (e.g., mean squared error or cross-validated R-squared). The lambda value that yields the best performance on the validation subsets is considered the optimal choice. Grid search involves systematically trying different lambda values and evaluating the model's performance to identify the lambda value that produces the best results. The optimal value of lambda depends on the specific dataset and problem at hand, and it balances the trade-off between model complexity and prediction accuracy.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
